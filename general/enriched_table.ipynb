{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e7e05c",
   "metadata": {},
   "source": [
    "# Generar tabla enriquecida uniendo las alertas high con las transacciones cash desde marzo de 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1cc4aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinso\\AppData\\Local\\Temp\\ipykernel_25792\\402389009.py:18: DtypeWarning: Columns (70,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tx = pd.read_csv(TX_PATH)                # suele venir con comas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas añadidas: ['tx_direction', 'tx_base_amount', 'customer_type', 'customer_account_balance', 'customer_networth', 'customer_income', 'customer_expected_amount', 'customer_sub_type']\n",
      "   alert_id rule_code  subject_ids  \\\n",
      "0     82271    AAD-LA        26375   \n",
      "1     83230     P-TLO        24618   \n",
      "2     83231  PGAV-OUT        24618   \n",
      "\n",
      "                                  subject_names  number_of_transactions  \\\n",
      "0  MOREL           BULICIC         JORGE RAFAEL                       2   \n",
      "1         GARCIA          LABORA          WALDO                       1   \n",
      "2         GARCIA          LABORA          WALDO                       1   \n",
      "\n",
      "                         created_at          status external_transaction_ids  \\\n",
      "0  2025-03-07 12:27:52.142219+00:00  Not Suspicious      201955096, 68816944   \n",
      "1  2025-03-11 12:33:06.388175+00:00  Not Suspicious                202249097   \n",
      "2  2025-03-11 12:33:06.388175+00:00  Not Suspicious                202249097   \n",
      "\n",
      "  tx_direction tx_base_amount customer_type customer_account_balance  \\\n",
      "0       NA, NA         NA, NA        NA, NA                   NA, NA   \n",
      "1     Outbound    230017346.0    Individual              366027051.0   \n",
      "2     Outbound    230017346.0    Individual              366027051.0   \n",
      "\n",
      "   customer_networth        customer_income customer_expected_amount  \\\n",
      "0             NA, NA                 NA, NA                   NA, NA   \n",
      "1  SIN CLASIFICACION  Entre 5 y 10 millones             1500000000.0   \n",
      "2  SIN CLASIFICACION  Entre 5 y 10 millones             1500000000.0   \n",
      "\n",
      "  customer_sub_type  \n",
      "0            NA, NA  \n",
      "1            Retail  \n",
      "2            Retail  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinso\\AppData\\Local\\Temp\\ipykernel_25792\\402389009.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({k: fn(g) for k, fn in aggs.items()}))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TX_PATH = \"../data/transacciones_cash_2025.csv\"\n",
    "ALERTS_PATH = \"../data/high_alerts_2025.csv\"\n",
    "OUT_PATH = \"../data/high_alerts_enriched_with_tx.csv\"\n",
    "\n",
    "# --- util: normalizar nombres de columnas ---\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out.columns = (out.columns\n",
    "                     .str.replace(\"\\ufeff\", \"\", regex=False)  # BOM si lo hubiera\n",
    "                     .str.strip()                              # espacios extremos\n",
    "                     .str.strip(\"\\\"'\")                         # comillas\n",
    "                     .str.rstrip(\";\"))                         # ; finales (como customer_sub_type;)\n",
    "    return out\n",
    "\n",
    "# --- leer CSVs ---\n",
    "tx = pd.read_csv(TX_PATH)                # suele venir con comas\n",
    "alerts = pd.read_csv(ALERTS_PATH, sep=\";\")  # este viene con ; por lo que indico sep\n",
    "\n",
    "tx = normalize_cols(tx)\n",
    "alerts = normalize_cols(alerts)\n",
    "\n",
    "# --- helpers para ids externos ---\n",
    "def _normalize_single_id(token) -> str:\n",
    "    if pd.isna(token): \n",
    "        return None\n",
    "    t = str(token).strip().strip(\"[](){}\\\"'\")\n",
    "    if t.endswith(\".0\"):\n",
    "        t = t[:-2]\n",
    "    return t\n",
    "\n",
    "def _split_external_ids(cell) -> list:\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    s = str(cell)\n",
    "    parts = s.split(\",\") if \",\" in s else [s]\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        norm = _normalize_single_id(p)\n",
    "        if norm:\n",
    "            out.append(norm)\n",
    "    return out\n",
    "\n",
    "# --- normalizar tx_id en transacciones para el join ---\n",
    "tx = tx.assign(\n",
    "    tx_id_norm=lambda d: d[\"tx_id\"].astype(str)\n",
    "                         .str.strip()\n",
    "                         .str.strip(\"[](){}\\\"'\")\n",
    "                         .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# columnas de interés a traer desde transacciones\n",
    "cols_tx_interes = [\n",
    "    \"tx_direction\",\n",
    "    \"tx_base_amount\",\n",
    "    \"customer_type\",\n",
    "    \"customer_account_balance\",\n",
    "    \"customer_networth\",\n",
    "    \"customer_income\",\n",
    "    \"customer_expected_amount\",\n",
    "    \"customer_sub_type\",\n",
    "]\n",
    "\n",
    "present = [c for c in cols_tx_interes if c in tx.columns]\n",
    "missing = [c for c in cols_tx_interes if c not in tx.columns]\n",
    "if missing:\n",
    "    print(\"Aviso: no se encontraron en transacciones las columnas ->\", missing)\n",
    "\n",
    "# --- desarmar ids externos preservando orden ---\n",
    "alerts = alerts.copy()\n",
    "alerts[\"ext_ids_list\"] = alerts[\"external_transaction_ids\"].apply(_split_external_ids)\n",
    "exploded = alerts.explode(\"ext_ids_list\", ignore_index=False)\n",
    "exploded[\"seq\"] = exploded.groupby(\"alert_id\").cumcount()\n",
    "\n",
    "# --- join (cada id externo -> fila de transacción Cash) ---\n",
    "ex_join = exploded.merge(\n",
    "    tx[[\"tx_id_norm\"] + present],\n",
    "    left_on=\"ext_ids_list\",\n",
    "    right_on=\"tx_id_norm\",\n",
    "    how=\"left\"  # si no está (probable Buy/Sell), quedará NA\n",
    ")\n",
    "\n",
    "# --- limpiador de valores y agregador con colapso inteligente ---\n",
    "def _clean_value(v) -> str:\n",
    "    \"\"\"Limpia cada valor: quita espacios, comillas y ';' finales. NaN -> 'NA'.\"\"\"\n",
    "    if pd.isna(v):\n",
    "        return \"NA\"\n",
    "    s = str(v).strip().strip(\"\\\"'\").rstrip(\";\")\n",
    "    return s\n",
    "\n",
    "def _collapse_or_join(df_group: pd.DataFrame, value_col: str) -> str:\n",
    "    \"\"\"\n",
    "    - Si TODOS los valores no-NA son iguales y no hay 'NA' -> devuelve ese único valor.\n",
    "    - En otro caso -> devuelve los valores limpios separados por coma\n",
    "      preservando el orden original (columna 'seq').\n",
    "    \"\"\"\n",
    "    vals = df_group.sort_values(\"seq\")[value_col].map(_clean_value).tolist()\n",
    "    non_na = [x for x in vals if x not in (\"\", \"NA\")]\n",
    "    if non_na and len(set(non_na)) == 1 and \"NA\" not in vals:\n",
    "        return non_na[0]\n",
    "    return \", \".join(vals)\n",
    "\n",
    "# --- agregar por alerta usando el nuevo agregador ---\n",
    "aggs = {col: (lambda g, c=col: _collapse_or_join(g, c)) for col in present}\n",
    "by_alert = (\n",
    "    ex_join.groupby(\"alert_id\", group_keys=False)\n",
    "           .apply(lambda g: pd.Series({k: fn(g) for k, fn in aggs.items()}))\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "# --- resultado final ---\n",
    "df_high_alerts_enriched = alerts.drop(columns=[\"ext_ids_list\"]).merge(by_alert, on=\"alert_id\", how=\"left\")\n",
    "\n",
    "# guardar\n",
    "df_high_alerts_enriched.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(\"Columnas añadidas:\", present)\n",
    "print(df_high_alerts_enriched.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9b362",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
